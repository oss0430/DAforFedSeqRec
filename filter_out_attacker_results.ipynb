{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Codes reads client wise evaluation results to generate a summary of the evaluation results, We exclude attacking node evaluation results for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_method: sr_targeted_segment_sasrec\n",
      "0 of attackers : []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "eval_folder_path = \"exp/FedAvg_sasrec_on_sr_data_lr0.001_lstep60/attn_5_hidden_4\"\n",
    "evaluation_log_path = os.path.join(eval_folder_path, \"eval_results.log\")\n",
    "config_path = os.path.join(eval_folder_path, \"config.yaml\")\n",
    "\n",
    "\n",
    "## read yaml for determine attack method and attacker node ids\n",
    "import yaml\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "attack_method = config[\"attack\"][\"attack_method\"]\n",
    "attacker_id = config[\"attack\"][\"attacker_id\"]\n",
    "\n",
    "print(f\"attack_method: {attack_method}\")\n",
    "print(f\"{len(attacker_id)} of attackers : {attacker_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 6040\n",
      "Example of client result: {'Role': 'Client #1', 'Round': 200, 'Results_raw': {'val_avg_loss': 8.41269588470459, 'val_total': 1, 'val_loss': 8.41269588470459, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_avg_loss': 8.343064308166504, 'test_total': 1, 'test_loss': 8.343064308166504, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}}\n",
      "total raw results counts are : 6040\n",
      "Number of benign client results: 6040\n",
      "Round 200\n",
      "Averaged metrics: {'val_avg_loss': 8.230483098219562, 'val_total': 1.0, 'val_loss': 8.230483098219562, 'val_recall_10': 0.029966887417218542, 'val_recall_20': 0.04205298013245033, 'val_ndcg_10': 0.013381423002764105, 'val_ndcg_20': 0.016358428313173964, 'test_avg_loss': 8.22618792135984, 'test_total': 1.0, 'test_loss': 8.22618792135984, 'test_recall_10': 0.03129139072847682, 'test_recall_20': 0.04370860927152318, 'test_ndcg_10': 0.013696672303238799, 'test_ndcg_20': 0.016768907496565778}\n",
      "Standard deviation of metrics: {'val_avg_loss': 675.3756770727123, 'val_total': 0.0, 'val_loss': 675.3756770727123, 'val_recall_10': 175.57599337749218, 'val_recall_20': 243.3185430463865, 'val_ndcg_10': 39.292649303306284, 'val_ndcg_20': 43.20371388581489, 'test_avg_loss': 660.5557404067474, 'test_total': 0.0, 'test_loss': 660.5557404067474, 'test_recall_10': 183.0859271523079, 'test_recall_20': 252.46092715234144, 'test_ndcg_10': 39.90569655367178, 'test_ndcg_20': 43.944921826295065}\n",
      "Min metrics: {'val_avg_loss': 5.608146667480469, 'val_total': 1, 'val_loss': 5.608146667480469, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_avg_loss': 5.633187294006348, 'test_total': 1, 'test_loss': 5.633187294006348, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}\n",
      "Max metrics: {'val_avg_loss': 9.318668365478516, 'val_total': 1, 'val_loss': 9.318668365478516, 'val_recall_10': 1.0, 'val_recall_20': 1.0, 'val_ndcg_10': 1.0, 'val_ndcg_20': 1.0, 'test_avg_loss': 9.293713569641113, 'test_total': 1, 'test_loss': 9.293713569641113, 'test_recall_10': 1.0, 'test_recall_20': 1.0, 'test_ndcg_10': 1.0, 'test_ndcg_20': 1.0}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read evaluation log\n",
    "## we only require every client's Result_raw for each round\n",
    "from collections import defaultdict\n",
    "\"\"\"\n",
    "    {'Role': 'Client #*', 'Round': '*' : 'Results_raw' : {}}\n",
    "\"\"\"\n",
    "## parse the log file line by line\n",
    "raw_client_results = []\n",
    "with open(evaluation_log_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"Client #\" in line:\n",
    "            ## read line as dict\n",
    "            result_dict = eval(line)\n",
    "            raw_client_results.append(result_dict)\n",
    "            \n",
    "print(f\"Number of clients: {len(raw_client_results)}\")\n",
    "print(f\"Example of client result: {raw_client_results[0]}\")\n",
    "\n",
    "## iterate over all clients and extract the Results_raw group by round\n",
    "## exclude attacker id if matched client id\n",
    "round_results = defaultdict(list)\n",
    "print(f\"total raw results counts are : {len(raw_client_results)}\")\n",
    "for client_result in raw_client_results:\n",
    "    current_round = client_result['Round']\n",
    "    client_id = int(client_result['Role'].split(\"#\")[1])\n",
    "    if client_id in attacker_id:\n",
    "        #print(client_id)\n",
    "        continue\n",
    "    else :\n",
    "        round_results[current_round].append(client_result[\"Results_raw\"])\n",
    "\n",
    "## first key\n",
    "first_key = list(round_results.keys())[0]\n",
    "\n",
    "print(f\"Number of benign client results: {len(round_results[first_key])}\")\n",
    "\n",
    "eval_keys = list(round_results[first_key][0].keys())\n",
    "## 1. Average all metrics\n",
    "## 2. std of all metrics\n",
    "## 3. Min/Max of all metrics\n",
    "\n",
    "\n",
    "for round, client_results in round_results.items():\n",
    "    print(f\"Round {round}\")\n",
    "    ## 1. Average all metrics\n",
    "    avg_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            avg_metrics[key] += client_result[key]\n",
    "    for key in eval_keys:\n",
    "        avg_metrics[key] /= len(client_results)\n",
    "    print(f\"Averaged metrics: {avg_metrics}\")\n",
    "    ## 2. std of all metrics\n",
    "    std_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            std_metrics[key] += (client_result[key] - avg_metrics[key]) ** 2\n",
    "    \n",
    "    print(f\"Standard deviation of metrics: {std_metrics}\")\n",
    "    \n",
    "    ## 3. Min/Max of all metrics\n",
    "    min_metrics = {key: float('inf') for key in eval_keys}\n",
    "    max_metrics = {key: float('-inf') for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            min_metrics[key] = min(min_metrics[key], client_result[key])\n",
    "            max_metrics[key] = max(max_metrics[key], client_result[key])\n",
    "    print(f\"Min metrics: {min_metrics}\")\n",
    "    print(f\"Max metrics: {max_metrics}\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
