{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Codes reads client wise evaluation results to generate a summary of the evaluation results, We exclude attacking node evaluation results for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_method: \n",
      "0 of attackers : []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "eval_folder_path = \"exp/FedAvg_sasrec_on_sr_data_lr0.01_lstep1/Hinge_augmented_4e-1_num_client_per_round_16\"\n",
    "evaluation_log_path = os.path.join(eval_folder_path, \"eval_results.log\")\n",
    "config_path = os.path.join(eval_folder_path, \"config.yaml\")\n",
    "\n",
    "\n",
    "## read yaml for determine attack method and attacker node ids\n",
    "import yaml\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "attack_method = config[\"attack\"][\"attack_method\"]\n",
    "attacker_id = config[\"attack\"][\"attacker_id\"]\n",
    "\n",
    "print(f\"attack_method: {attack_method}\")\n",
    "print(f\"{len(attacker_id)} of attackers : {attacker_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 6040\n",
      "Example of client result: {'Role': 'Client #1', 'Round': 2500, 'Results_raw': {'val_loss': 0.0, 'val_total': 1, 'val_avg_loss': 0.0, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_loss': 0.0, 'test_total': 1, 'test_avg_loss': 0.0, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}}\n",
      "total raw results counts are : 6040\n",
      "Number of benign client results: 6040\n",
      "Round 2500\n",
      "Averaged metrics: {'val_loss': 0.00048680179166478037, 'val_total': 1.0, 'val_avg_loss': 0.00048680179166478037, 'val_recall_10': 0.026158940397350994, 'val_recall_20': 0.042549668874172185, 'val_ndcg_10': 0.015102074408829356, 'val_ndcg_20': 0.019094362207633536, 'test_loss': 0.0003589955386736535, 'test_total': 1.0, 'test_avg_loss': 0.0003589955386736535, 'test_recall_10': 0.02599337748344371, 'test_recall_20': 0.042549668874172185, 'test_ndcg_10': 0.01483803495091027, 'test_ndcg_20': 0.01891916098641517}\n",
      "Standard deviation of metrics: {'val_loss': 1.5268556516665612, 'val_total': 0.0, 'val_avg_loss': 1.5268556516665612, 'val_recall_10': 153.86688741722188, 'val_recall_20': 246.06473509934472, 'val_ndcg_10': 63.91952512203083, 'val_ndcg_20': 68.98646809699422, 'test_loss': 0.6076863060300971, 'test_total': 0.0, 'test_avg_loss': 0.6076863060300971, 'test_recall_10': 152.91903973509912, 'test_recall_20': 246.06473509934457, 'test_ndcg_10': 61.38892115082579, 'test_ndcg_20': 66.65160705285874}\n",
      "Min metrics: {'val_loss': 0.0, 'val_total': 1, 'val_avg_loss': 0.0, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_loss': 0.0, 'test_total': 1, 'test_avg_loss': 0.0, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}\n",
      "Max metrics: {'val_loss': 0.8316260576248169, 'val_total': 1, 'val_avg_loss': 0.8316260576248169, 'val_recall_10': 1.0, 'val_recall_20': 1.0, 'val_ndcg_10': 1.0, 'val_ndcg_20': 1.0, 'test_loss': 0.7105118632316589, 'test_total': 1, 'test_avg_loss': 0.7105118632316589, 'test_recall_10': 1.0, 'test_recall_20': 1.0, 'test_ndcg_10': 1.0, 'test_ndcg_20': 1.0}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read evaluation log\n",
    "## we only require every client's Result_raw for each round\n",
    "from collections import defaultdict\n",
    "\"\"\"\n",
    "    {'Role': 'Client #*', 'Round': '*' : 'Results_raw' : {}}\n",
    "\"\"\"\n",
    "## parse the log file line by line\n",
    "raw_client_results = []\n",
    "with open(evaluation_log_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"Client #\" in line:\n",
    "            ## read line as dict\n",
    "            result_dict = eval(line)\n",
    "            raw_client_results.append(result_dict)\n",
    "            \n",
    "print(f\"Number of clients: {len(raw_client_results)}\")\n",
    "print(f\"Example of client result: {raw_client_results[0]}\")\n",
    "\n",
    "## iterate over all clients and extract the Results_raw group by round\n",
    "## exclude attacker id if matched client id\n",
    "round_results = defaultdict(list)\n",
    "print(f\"total raw results counts are : {len(raw_client_results)}\")\n",
    "for client_result in raw_client_results:\n",
    "    current_round = client_result['Round']\n",
    "    client_id = int(client_result['Role'].split(\"#\")[1])\n",
    "    if client_id in attacker_id:\n",
    "        #print(client_id)\n",
    "        continue\n",
    "    else :\n",
    "        round_results[current_round].append(client_result[\"Results_raw\"])\n",
    "\n",
    "## first key\n",
    "first_key = list(round_results.keys())[0]\n",
    "\n",
    "print(f\"Number of benign client results: {len(round_results[first_key])}\")\n",
    "\n",
    "eval_keys = list(round_results[first_key][0].keys())\n",
    "## 1. Average all metrics\n",
    "## 2. std of all metrics\n",
    "## 3. Min/Max of all metrics\n",
    "\n",
    "\n",
    "for round, client_results in round_results.items():\n",
    "    print(f\"Round {round}\")\n",
    "    ## 1. Average all metrics\n",
    "    avg_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            avg_metrics[key] += client_result[key]\n",
    "    for key in eval_keys:\n",
    "        avg_metrics[key] /= len(client_results)\n",
    "    print(f\"Averaged metrics: {avg_metrics}\")\n",
    "    ## 2. std of all metrics\n",
    "    std_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            std_metrics[key] += (client_result[key] - avg_metrics[key]) ** 2\n",
    "    \n",
    "    print(f\"Standard deviation of metrics: {std_metrics}\")\n",
    "    \n",
    "    ## 3. Min/Max of all metrics\n",
    "    min_metrics = {key: float('inf') for key in eval_keys}\n",
    "    max_metrics = {key: float('-inf') for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            min_metrics[key] = min(min_metrics[key], client_result[key])\n",
    "            max_metrics[key] = max(max_metrics[key], client_result[key])\n",
    "    print(f\"Min metrics: {min_metrics}\")\n",
    "    print(f\"Max metrics: {max_metrics}\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
