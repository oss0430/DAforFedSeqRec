{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Codes reads client wise evaluation results to generate a summary of the evaluation results, We exclude attacking node evaluation results for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_method: \n",
      "0 of attackers : []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "eval_folder_path = \"exp/FedAvg_sasrec_on_sr_data_lr0.001_lstep1/CE_ML-1M_rand_aug_1e-1\"\n",
    "evaluation_log_path = os.path.join(eval_folder_path, \"eval_results.log\")\n",
    "config_path = os.path.join(eval_folder_path, \"config.yaml\")\n",
    "\n",
    "\n",
    "## read yaml for determine attack method and attacker node ids\n",
    "import yaml\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "attack_method = config[\"attack\"][\"attack_method\"]\n",
    "attacker_id = config[\"attack\"][\"attacker_id\"]\n",
    "\n",
    "print(f\"attack_method: {attack_method}\")\n",
    "print(f\"{len(attacker_id)} of attackers : {attacker_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 6040\n",
      "Example of client result: {'Role': 'Client #1', 'Round': 4000, 'Results_raw': {'val_loss': 7.307229995727539, 'val_total': 1, 'val_avg_loss': 7.307229995727539, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_loss': 9.138725280761719, 'test_total': 1, 'test_avg_loss': 9.138725280761719, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}}\n",
      "total raw results counts are : 6040\n",
      "Number of benign client results: 6040\n",
      "Round 4000\n",
      "Averaged metrics: {'val_loss': 8.093041948589269, 'val_total': 1.0, 'val_avg_loss': 8.093041948589269, 'val_recall_10': 0.041721854304635764, 'val_recall_20': 0.06556291390728476, 'val_ndcg_10': 0.017070472640114905, 'val_ndcg_20': 0.022953054839313425, 'test_loss': 8.085947570184999, 'test_total': 1.0, 'test_avg_loss': 8.085947570184999, 'test_recall_10': 0.043874172185430466, 'test_recall_20': 0.06771523178807946, 'test_ndcg_10': 0.01762662407084806, 'test_ndcg_20': 0.02345891097276811}\n",
      "Standard deviation of metrics: {'val_loss': 13445.970654418785, 'val_total': 0.0, 'val_avg_loss': 13445.970654418785, 'val_recall_10': 241.48609271525365, 'val_recall_20': 370.0370860927356, 'val_ndcg_10': 43.39774908789547, 'val_ndcg_20': 50.77637906144304, 'test_loss': 13138.737635357591, 'test_total': 0.0, 'test_avg_loss': 13138.737635357591, 'test_recall_10': 253.3733443709005, 'test_recall_20': 381.3044701986987, 'test_ndcg_10': 43.6837836462256, 'test_ndcg_20': 50.886409285959225}\n",
      "Min metrics: {'val_loss': 1.98623788356781, 'val_total': 1, 'val_avg_loss': 1.98623788356781, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_loss': 1.8628942966461182, 'test_total': 1, 'test_avg_loss': 1.8628942966461182, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}\n",
      "Max metrics: {'val_loss': 14.876235961914062, 'val_total': 1, 'val_avg_loss': 14.876235961914062, 'val_recall_10': 1.0, 'val_recall_20': 1.0, 'val_ndcg_10': 0.6309297680854797, 'val_ndcg_20': 0.6309297680854797, 'test_loss': 14.0732421875, 'test_total': 1, 'test_avg_loss': 14.0732421875, 'test_recall_10': 1.0, 'test_recall_20': 1.0, 'test_ndcg_10': 0.6309297680854797, 'test_ndcg_20': 0.6309297680854797}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read evaluation log\n",
    "## we only require every client's Result_raw for each round\n",
    "from collections import defaultdict\n",
    "\"\"\"\n",
    "    {'Role': 'Client #*', 'Round': '*' : 'Results_raw' : {}}\n",
    "\"\"\"\n",
    "## parse the log file line by line\n",
    "raw_client_results = []\n",
    "with open(evaluation_log_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"Client #\" in line:\n",
    "            ## read line as dict\n",
    "            result_dict = eval(line)\n",
    "            raw_client_results.append(result_dict)\n",
    "            \n",
    "print(f\"Number of clients: {len(raw_client_results)}\")\n",
    "print(f\"Example of client result: {raw_client_results[0]}\")\n",
    "\n",
    "## iterate over all clients and extract the Results_raw group by round\n",
    "## exclude attacker id if matched client id\n",
    "round_results = defaultdict(list)\n",
    "print(f\"total raw results counts are : {len(raw_client_results)}\")\n",
    "for client_result in raw_client_results:\n",
    "    current_round = client_result['Round']\n",
    "    client_id = int(client_result['Role'].split(\"#\")[1])\n",
    "    if client_id in attacker_id:\n",
    "        #print(client_id)\n",
    "        continue\n",
    "    else :\n",
    "        round_results[current_round].append(client_result[\"Results_raw\"])\n",
    "\n",
    "## first key\n",
    "first_key = list(round_results.keys())[0]\n",
    "\n",
    "print(f\"Number of benign client results: {len(round_results[first_key])}\")\n",
    "\n",
    "eval_keys = list(round_results[first_key][0].keys())\n",
    "## 1. Average all metrics\n",
    "## 2. std of all metrics\n",
    "## 3. Min/Max of all metrics\n",
    "\n",
    "\n",
    "for round, client_results in round_results.items():\n",
    "    print(f\"Round {round}\")\n",
    "    ## 1. Average all metrics\n",
    "    avg_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            avg_metrics[key] += client_result[key]\n",
    "    for key in eval_keys:\n",
    "        avg_metrics[key] /= len(client_results)\n",
    "    print(f\"Averaged metrics: {avg_metrics}\")\n",
    "    ## 2. std of all metrics\n",
    "    std_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            std_metrics[key] += (client_result[key] - avg_metrics[key]) ** 2\n",
    "    \n",
    "    print(f\"Standard deviation of metrics: {std_metrics}\")\n",
    "    \n",
    "    ## 3. Min/Max of all metrics\n",
    "    min_metrics = {key: float('inf') for key in eval_keys}\n",
    "    max_metrics = {key: float('-inf') for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            min_metrics[key] = min(min_metrics[key], client_result[key])\n",
    "            max_metrics[key] = max(max_metrics[key], client_result[key])\n",
    "    print(f\"Min metrics: {min_metrics}\")\n",
    "    print(f\"Max metrics: {max_metrics}\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
