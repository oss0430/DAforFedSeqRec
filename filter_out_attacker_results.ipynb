{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Codes reads client wise evaluation results to generate a summary of the evaluation results, We exclude attacking node evaluation results for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_method: sr_targeted_segment_sasrec\n",
      "0 of attackers : []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "eval_folder_path = \"exp/FedAvg_sasrec_on_sr_data_lr0.001_lstep60/attn8e-1_hidden8e-1\"\n",
    "evaluation_log_path = os.path.join(eval_folder_path, \"eval_results.log\")\n",
    "config_path = os.path.join(eval_folder_path, \"config.yaml\")\n",
    "\n",
    "\n",
    "## read yaml for determine attack method and attacker node ids\n",
    "import yaml\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "attack_method = config[\"attack\"][\"attack_method\"]\n",
    "attacker_id = config[\"attack\"][\"attacker_id\"]\n",
    "\n",
    "print(f\"attack_method: {attack_method}\")\n",
    "print(f\"{len(attacker_id)} of attackers : {attacker_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 6040\n",
      "Example of client result: {'Role': 'Client #1', 'Round': 200, 'Results_raw': {'val_avg_loss': 8.372604370117188, 'val_loss': 8.372604370117188, 'val_total': 1, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_avg_loss': 8.493143081665039, 'test_loss': 8.493143081665039, 'test_total': 1, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}}\n",
      "total raw results counts are : 6040\n",
      "Number of benign client results: 6040\n",
      "Round 200\n",
      "Averaged metrics: {'val_avg_loss': 8.172128115505572, 'val_loss': 8.172128115505572, 'val_total': 1.0, 'val_recall_10': 0.038410596026490065, 'val_recall_20': 0.05496688741721854, 'val_ndcg_10': 0.01765746839491343, 'val_ndcg_20': 0.021756006579477526, 'test_avg_loss': 8.166678381834599, 'test_loss': 8.166678381834599, 'test_total': 1.0, 'test_recall_10': 0.03956953642384106, 'test_recall_20': 0.05827814569536424, 'test_ndcg_10': 0.01873728401519899, 'test_ndcg_20': 0.023324751754615566}\n",
      "Standard deviation of metrics: {'val_avg_loss': 572.9315801748647, 'val_loss': 572.9315801748647, 'val_total': 0.0, 'val_recall_10': 223.08874172182934, 'val_recall_20': 313.7509933774779, 'val_ndcg_10': 56.07222651108322, 'val_ndcg_20': 61.2483648104087, 'test_avg_loss': 574.759047899694, 'test_loss': 574.759047899694, 'test_total': 0.0, 'test_recall_10': 229.54288079466778, 'test_recall_20': 331.48609271525675, 'test_ndcg_10': 61.89029857661421, 'test_ndcg_20': 67.54236412080004}\n",
      "Min metrics: {'val_avg_loss': 6.517762660980225, 'val_loss': 6.517762660980225, 'val_total': 1, 'val_recall_10': 0.0, 'val_recall_20': 0.0, 'val_ndcg_10': 0.0, 'val_ndcg_20': 0.0, 'test_avg_loss': 6.535710334777832, 'test_loss': 6.535710334777832, 'test_total': 1, 'test_recall_10': 0.0, 'test_recall_20': 0.0, 'test_ndcg_10': 0.0, 'test_ndcg_20': 0.0}\n",
      "Max metrics: {'val_avg_loss': 8.927421569824219, 'val_loss': 8.927421569824219, 'val_total': 1, 'val_recall_10': 1.0, 'val_recall_20': 1.0, 'val_ndcg_10': 1.0, 'val_ndcg_20': 1.0, 'test_avg_loss': 8.987412452697754, 'test_loss': 8.987412452697754, 'test_total': 1, 'test_recall_10': 1.0, 'test_recall_20': 1.0, 'test_ndcg_10': 1.0, 'test_ndcg_20': 1.0}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read evaluation log\n",
    "## we only require every client's Result_raw for each round\n",
    "from collections import defaultdict\n",
    "\"\"\"\n",
    "    {'Role': 'Client #*', 'Round': '*' : 'Results_raw' : {}}\n",
    "\"\"\"\n",
    "## parse the log file line by line\n",
    "raw_client_results = []\n",
    "with open(evaluation_log_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"Client #\" in line:\n",
    "            ## read line as dict\n",
    "            result_dict = eval(line)\n",
    "            raw_client_results.append(result_dict)\n",
    "            \n",
    "print(f\"Number of clients: {len(raw_client_results)}\")\n",
    "print(f\"Example of client result: {raw_client_results[0]}\")\n",
    "\n",
    "## iterate over all clients and extract the Results_raw group by round\n",
    "## exclude attacker id if matched client id\n",
    "round_results = defaultdict(list)\n",
    "print(f\"total raw results counts are : {len(raw_client_results)}\")\n",
    "for client_result in raw_client_results:\n",
    "    current_round = client_result['Round']\n",
    "    client_id = int(client_result['Role'].split(\"#\")[1])\n",
    "    if client_id in attacker_id:\n",
    "        #print(client_id)\n",
    "        continue\n",
    "    else :\n",
    "        round_results[current_round].append(client_result[\"Results_raw\"])\n",
    "\n",
    "## first key\n",
    "first_key = list(round_results.keys())[0]\n",
    "\n",
    "print(f\"Number of benign client results: {len(round_results[first_key])}\")\n",
    "\n",
    "eval_keys = list(round_results[first_key][0].keys())\n",
    "## 1. Average all metrics\n",
    "## 2. std of all metrics\n",
    "## 3. Min/Max of all metrics\n",
    "\n",
    "\n",
    "for round, client_results in round_results.items():\n",
    "    print(f\"Round {round}\")\n",
    "    ## 1. Average all metrics\n",
    "    avg_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            avg_metrics[key] += client_result[key]\n",
    "    for key in eval_keys:\n",
    "        avg_metrics[key] /= len(client_results)\n",
    "    print(f\"Averaged metrics: {avg_metrics}\")\n",
    "    ## 2. std of all metrics\n",
    "    std_metrics = {key: 0.0 for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            std_metrics[key] += (client_result[key] - avg_metrics[key]) ** 2\n",
    "    \n",
    "    print(f\"Standard deviation of metrics: {std_metrics}\")\n",
    "    \n",
    "    ## 3. Min/Max of all metrics\n",
    "    min_metrics = {key: float('inf') for key in eval_keys}\n",
    "    max_metrics = {key: float('-inf') for key in eval_keys}\n",
    "    for client_result in client_results:\n",
    "        for key in eval_keys:\n",
    "            min_metrics[key] = min(min_metrics[key], client_result[key])\n",
    "            max_metrics[key] = max(max_metrics[key], client_result[key])\n",
    "    print(f\"Min metrics: {min_metrics}\")\n",
    "    print(f\"Max metrics: {max_metrics}\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
