{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First model loss:  0.4339907467365265\n",
      "Second model loss:  0.40908360481262207\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TestModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(2, 2)\n",
    "        self.fc2 = torch.nn.Linear(2, 1)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "test_input = torch.tensor([[1.0, 1.0], [0.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "test_label = torch.tensor([[0.0], [0.0], [1.0], [1.0]])\n",
    "\n",
    "first_model = TestModel()\n",
    "second_model = TestModel()\n",
    "\n",
    "first_loss = loss(first_model(test_input), test_label)\n",
    "second_loss = loss(second_model(test_input), test_label)\n",
    "\n",
    "print(\"First model loss: \", first_loss.item())\n",
    "print(\"Second model loss: \", second_loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "## Random Gaussian Noise for Dataset\n",
    "data = torch.normal(0, 1, (50, 2))\n",
    "## 1 or 0 as labels\n",
    "labels = torch.randint(0, 2, (50, 1)).float()\n",
    "                      \n",
    "original_dataset = ReconstructionDataset(data, labels)\n",
    "original_dataloader = torch.utils.data.DataLoader(original_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "training_rate = 0.01 * 60\n",
    "reconstruction_iter = 10\n",
    "\n",
    "def original_reconstruct_function(model_0, model_1) :\n",
    "    \n",
    "    model_0_params = dict(model_0.named_parameters())\n",
    "    model_1_params = dict(model_1.named_parameters())\n",
    "    \n",
    "    mean_gradient_per_param ={}\n",
    "    for name, param1 in model_0_params.items():\n",
    "        param2 = model_1_params[name]\n",
    "        parameter_diff = (param2 - param1)\n",
    "        mean_gradient = parameter_diff / training_rate\n",
    "        mean_gradient_per_param[name] = mean_gradient.detach()\n",
    "        \n",
    "    dataloader = copy.deepcopy(original_dataloader)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    for data, label in dataloader:\n",
    "        \n",
    "        generated_data = data.clone().detach()\n",
    "        generated_data.requires_grad = True\n",
    "        optimizer = torch.optim.SGD([generated_data], lr=0.01)\n",
    "        def closure():\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            output = model_1(generated_data)\n",
    "            loss = criterion(output, label)\n",
    "            dummy_gradient = torch.autograd.grad(loss, model_1.parameters(), create_graph=True)\n",
    "            \n",
    "            l2_loss = 0\n",
    "            for dummy_grad, mean_grad in zip(dummy_gradient, mean_gradient_per_param.values()):\n",
    "                l2_loss += ((dummy_grad - mean_grad) ** 2).sum()             ## float32 - float32\n",
    "            l2_loss.backward()\n",
    "\n",
    "            return l2_loss\n",
    "        \n",
    "        for i in range(reconstruction_iter):\n",
    "            optimizer.step(closure)\n",
    "            #print(f\"{i}th l2 loss : {closure()}\")\n",
    "            \n",
    "\n",
    "def vertical_l2_loss_reconstruct_function(model_0, model_1) :\n",
    "    \n",
    "    model_0_params = dict(model_0.named_parameters())\n",
    "    model_1_params = dict(model_1.named_parameters())\n",
    "    \n",
    "    mean_gradient_per_param ={}\n",
    "    for name, param1 in model_0_params.items():\n",
    "        param2 = model_1_params[name]\n",
    "        parameter_diff = (param2 - param1)\n",
    "        mean_gradient = parameter_diff / training_rate\n",
    "        mean_gradient_per_param[name] = mean_gradient.detach()\n",
    "    \n",
    "    stacked_mean_gradient = torch.stack(list(mean_gradient_per_param.values()))\n",
    "        \n",
    "    dataloader = copy.deepcopy(original_dataloader)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    for data, label in dataloader:\n",
    "        \n",
    "        generated_data = data.clone().detach()\n",
    "        generated_data.requires_grad = True\n",
    "        optimizer = torch.optim.SGD([generated_data], lr=0.01)\n",
    "        def closure():\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            output = model_1(generated_data)\n",
    "            loss = criterion(output, label)\n",
    "            dummy_gradient = torch.autograd.grad(loss, model_1.parameters(), create_graph=True)\n",
    "            stacked_dummpy_gradient = torch.stack(dummy_gradient)\n",
    "            \n",
    "            l2_loss = ((stacked_dummpy_gradient - stacked_mean_gradient) ** 2).sum()\n",
    "            \n",
    "            l2_loss.backward()\n",
    "\n",
    "            return l2_loss\n",
    "        \n",
    "        for i in range(reconstruction_iter):\n",
    "            optimizer.step(closure)\n",
    "            #print(f\"{i}th l2 loss : {closure()}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 2] at entry 0 and [2] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_381919/3914504398.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mreconstruction_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvertical_l2_loss_reconstruct_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mreconstruction_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mvertical_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruction_end_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreconstruction_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_381919/2173743575.py\u001b[0m in \u001b[0;36mvertical_l2_loss_reconstruct_function\u001b[0;34m(model_0, model_1)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mmean_gradient_per_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mstacked_mean_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_gradient_per_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 2] at entry 0 and [2] at entry 1"
     ]
    }
   ],
   "source": [
    "## time the reconstruction of the input\n",
    "import time\n",
    "\n",
    "reconstruction_start_time = time.time()\n",
    "\n",
    "original_reconstruct_function(first_model, second_model)\n",
    "reconstruction_end_time = time.time()\n",
    "original_time = reconstruction_end_time - reconstruction_start_time\n",
    "#print(f\"Reconstruction time: {reconstruction_end_time - reconstruction_start_time}\")\n",
    "\n",
    "reconstruction_start_time = time.time()\n",
    "vertical_l2_loss_reconstruct_function(first_model, second_model)\n",
    "reconstruction_end_time = time.time()\n",
    "vertical_time = reconstruction_end_time - reconstruction_start_time\n",
    "\n",
    "time_diff = original_time - vertical_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_381919/1382356152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def reconstruct_sequence_embedding_via_label(self,\n\u001b[0;32m----> 2\u001b[0;31m                                             \u001b[0mlabels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                                             \u001b[0mlist_of_item_seq_len\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                             \u001b[0mcurrent_model\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             \u001b[0mprevious_model\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def reconstruct_sequence_embedding_via_label(self,\n",
    "                                            labels : List[torch.Tensor],\n",
    "                                            list_of_item_seq_len : List[torch.Tensor],\n",
    "                                            current_model : torch.nn.Module,\n",
    "                                            previous_model : torch.nn.Module,\n",
    "                                            epoch : int,\n",
    "                                            learning_rate : float,\n",
    "                                            criterion_type : str,\n",
    "                                            record_history : bool = False) -> Dict[torch.Tensor, Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        reconstruct gaussian noise to match the right x_embedding for label\n",
    "        \n",
    "        The original Deep Leakage easedrops on normal gradient, and infer the training data.\n",
    "        In this algorithm we make assumptions that normal gradient is the difference between\n",
    "        current model and previous model.\n",
    "        \"\"\"\n",
    "        \n",
    "        generated_data = {\"input_embedding\" : [],\n",
    "                          \"item_seq_len\" : [],\n",
    "                          \"original_label\" : [],\n",
    "                          \"target_item\" : [],\n",
    "                          \"history\" : []}\n",
    "        training_rate = epoch * learning_rate\n",
    "        \n",
    "        previous_model_params = dict(previous_model.named_parameters())\n",
    "        current_model_params = dict(current_model.named_parameters())\n",
    "        \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        mean_gradient_per_param ={}\n",
    "        for name, param1 in previous_model_params.items():\n",
    "            param2 = current_model_params[name]\n",
    "            parameter_diff = (param2 - param1)\n",
    "            mean_gradient = parameter_diff / training_rate\n",
    "            mean_gradient_per_param[name] = mean_gradient.detach()\n",
    "        \n",
    "        reconstruction_loader = self._get_reconstruction_loader(labels, self._cfg.attack.reconstruction_batch_size)\n",
    "        \n",
    "        ## per batch generate\n",
    "        for batch in reconstruction_loader:\n",
    "            label = batch['label']\n",
    "            input_embedding = batch['input_embedding']\n",
    "            item_seq_len = batch['item_seq_len']\n",
    "            \n",
    "            generated_input_embedding = input_embedding.clone().detach()\n",
    "            generated_input_embedding.requires_grad_(True)\n",
    "            \n",
    "            optimizer = torch.optim.LBFGS([generated_input_embedding])\n",
    "            \n",
    "            history = {}\n",
    "            ## initialize history section via labels\n",
    "            for single_label in label:\n",
    "                history[single_label.item()] = {\"input_embedding\" : [], \"loss\" : []}\n",
    "                \n",
    "            for iter in range(self._cfg.attack.reconstruction_iter):\n",
    "                def closure():\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = current_model.embedding_forward(generated_input_embedding, item_seq_len)\n",
    "                    test_item_emb = current_model.item_embedding.weight\n",
    "                    logits = torch.matmul(outputs, test_item_emb.transpose(0,1))\n",
    "                    loss = criterion(logits, label)\n",
    "                    dummy_gradient = torch.autograd.grad(loss, current_model.parameters(), create_graph=True) ## Tuple of tensors\n",
    "                    \n",
    "                    l2_loss = 0\n",
    "                    ## Since this is mean_gradient_per_param is dict it value is the tensor.\n",
    "                    for dummy_grad, mean_grad in zip(dummy_gradient, mean_gradient_per_param.values()):\n",
    "                        l2_loss += ((dummy_grad - mean_grad) ** 2).sum()\n",
    "                        ## float32 - float32\n",
    "                    l2_loss.backward()\n",
    "                    \n",
    "                    return l2_loss\n",
    "                    \n",
    "                optimizer.step(closure)\n",
    "                \n",
    "                ## Try to Save every 10 iter\n",
    "                if (iter + 1) % 10 == 0 and record_history:\n",
    "                    current_grad_diff = closure()\n",
    "                    for single_label in label:\n",
    "                        history[single_label.item()][\"input_embedding\"].append(generated_input_embedding.clone().detach())\n",
    "                        history[single_label.item()][\"loss\"].append(current_grad_diff.clone().detach())\n",
    "                    \n",
    "            ## decompose the batch into single instance and\n",
    "            ## append to the generated_data\n",
    "            for i in range(len(label)):\n",
    "                generated_data[\"input_embedding\"].append(generated_input_embedding[i].unsqueeze(0).clone().detach()) # List[torch.Tensor]\n",
    "                generated_data[\"item_seq_len\"].append(item_seq_len[i]) # List[torch.Tensor]\n",
    "                generated_data[\"original_label\"].append(label[i]) # List[torch.Tensor]\n",
    "                generated_data[\"target_item\"].append(torch.zeros_like(item_seq_len[i]) + self._cfg.attack.target_item_id) # List[torch.Tensor]\n",
    "                generated_data[\"history\"].append(history[label[i].item()]) # List[Dict[str, List[torch.Tensor]]]\n",
    "       \n",
    "        \n",
    "        return generated_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
